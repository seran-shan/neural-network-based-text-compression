{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Neural Network-Based Text Compression\n",
    "\n",
    "#### üñãÔ∏è Authors\n",
    "- Feidnand Eide\n",
    "- Seran Shanmugathas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Install Libraries\n",
    "We will need the following libraries:\n",
    "- `pytorch`\n",
    "- `pytorch-lightning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-lightning --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Import Dependencies\n",
    "The following libraries are used in this project:\n",
    "- Standard libraries: `enum`, `ast`\n",
    "- PyTorch and PyTorch Lightning for model building and training\n",
    "- Transformers from Hugging Face for NLP tasks\n",
    "- Pandas for data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "Set up the configuration for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict = {\n",
    "    \"data_path\": \"data/uncompressed_and_compressed.csv\",\n",
    "    \"batch_size\": 32,\n",
    "    \"max_length\": 512,\n",
    "    \"vocab_size\": 256,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"max_epochs\": 10,\n",
    "    \"gpus\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Load and Preprocess the Dataset\n",
    "We define a custom dataset class for handling our text compression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Columns(Enum):\n",
    "    \"\"\"\n",
    "    Enum containing the columns of the dataset.\n",
    "    \"\"\"\n",
    "    UNCOMPRESSED = \"uncompressed\"\n",
    "    COMPRESSED = \"compressed\"\n",
    "\n",
    "class TextCompressionDataset(Dataset):\n",
    "    def __init__(self, uncompressed_texts, compressed_texts):\n",
    "        self.uncompressed_texts = uncompressed_texts\n",
    "        self.compressed_texts = compressed_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uncompressed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert each character in uncompressed text to its corresponding integer value\n",
    "        uncompressed_text = [ord(c) for c in self.uncompressed_texts[idx]]\n",
    "        uncompressed_text_tensor = torch.tensor(uncompressed_text, dtype=torch.long)\n",
    "\n",
    "        # Handle compressed text\n",
    "        compressed_text = self.compressed_texts[idx]\n",
    "        if isinstance(compressed_text, str):\n",
    "            # If it's a string representation of bytes, convert it to actual bytes\n",
    "            compressed_text = bytes(compressed_text, encoding='latin1')\n",
    "        compressed_text = [b for b in compressed_text]\n",
    "        compressed_text_tensor = torch.tensor(compressed_text, dtype=torch.long)\n",
    "        return uncompressed_text_tensor, compressed_text_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    uncompressed_texts, compressed_texts = zip(*batch)\n",
    "\n",
    "    max_seq_length = min(\n",
    "        max(len(t) for t in uncompressed_texts), max(len(t) for t in compressed_texts)\n",
    "    )\n",
    "\n",
    "    uncompressed_texts_padded = pad_sequence(\n",
    "        [t[:max_seq_length] for t in uncompressed_texts],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    )\n",
    "\n",
    "    # Truncate or pad the compressed texts\n",
    "    compressed_texts_padded = pad_sequence(\n",
    "        [t[:max_seq_length] for t in compressed_texts],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    )\n",
    "\n",
    "    return uncompressed_texts_padded, compressed_texts_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size=32, collate_fn=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.collate_fn = collate_fn\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Here you could split your dataset into train, val, and test sets if needed\n",
    "        # For example:\n",
    "        # self.train, self.val, self.test = random_split(self.dataset, [sizes])\n",
    "\n",
    "        # If you don't need to split, just assign the dataset to self.train\n",
    "        self.train = self.dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    # Implement val_dataloader and test_dataloader if you have validation and test sets\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.val, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ The Model\n",
    "Here we define our LSTM-based compression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMTextCompressor(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(LSTMTextCompressor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim).to(device)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.linear(lstm_out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        uncompressed_text, compressed_text = batch\n",
    "        output = self(uncompressed_text)\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            output.view(-1, self.vocab_size), compressed_text.view(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    def train_dataloader(self, dataset):\n",
    "        return DataLoader(\n",
    "            dataset, batch_size=32, shuffle=True, collate_fn=collate_batch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training\n",
    "Setting up the training environment and initiating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 65.5 K\n",
      "1 | lstm      | LSTM      | 1.1 M \n",
      "2 | linear    | Linear    | 65.8 K\n",
      "----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.736     Total estimated model params size (MB)\n",
      "/Users/seranshanmugathas/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8bcb431406480781abc39e51fb14d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(config[\"data_path\"], delimiter=\";\")\n",
    "\n",
    "dataset = TextCompressionDataset(\n",
    "    uncompressed_texts=df[\"uncompressed\"].values,\n",
    "    compressed_texts=df[\"compressed\"].values,\n",
    ")\n",
    "text_datamodule = TextDataModule(dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "model = LSTMTextCompressor(\n",
    "    vocab_size=config[\"vocab_size\"],\n",
    "    hidden_dim=config[\"hidden_dim\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=\"auto\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, text_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
